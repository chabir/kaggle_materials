Metrics
  1. AUC: use rank of probabilities before averaging submission not proba themselves
          donâ€™t need all raws of dataset (AUC is equivalent to a rank) especially if a category is not present in test  
  2. if target of regression is between 0 and 1, try Xentropy with RMSE.
  3. if target of regression is not between 0 and 1, try multiple binary folders.


Feature engineering for time series:

  2. type: count, cumcount (NA with mean), time delta (replace NA with max) (talkingdata-adtracking-fraud-detection), moving average 
  4. Time delta: be careful of timing test / train: readjust calculation of features to a day window if too many NAs
  5. keep an eye on duplicates for patterns (imagine how dataset was build)
  6. cluster on text variable or numeric variable after grouping on categorical var and grouping-by this '_cluster', calculate aggregated features like price rank: https://www.kaggle.com/c/avito-demand-prediction/discussion/59886
  7. reorder raws and columns to find time series (see santander 2018 comp) using brute force, jaccard index & graph, unique values & matrices
  
Feature engineering traditional:
  1. count NA, count NA per categorical / numerical /text var, count duplicates (even if removed later)
  2. if feature is a item seq, find the var that changes and the differences of this var as feature.
  3. Groupby, count,... :see TalkingData kernels for pandas possibilities
  4. anonymous data: try extratrees special tool.
  5. ideas: https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/18754/feature-engineering
  6. Noisy data: try DAE NN from seguro competition
  
  
Feature selection: 
LGB: personally trusted a lot on how LightGBM prioritises features, and I operate in the following principles:
      whenever a batch of features improve local validation score, keep them
      whenever a batch of feature decrease score - I will remove the most import features among this batch according to LightGBM, and re-test
      whenever features were not used by LightGBM at all i.e. 0 importance score I will drop them.
Time series: 
  1. try anokas tool
  2.


Categorical variable:
# https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63  for high cardinal


Large Datasets Tools:
1. Dask: pandas based
2. Big Query: Google SQL (fast!)
3. memo (see talking data competition)

Cross Validation:
https://www.kaggle.com/forums/f/15/kaggle-forum/t/18793/cross-validation-strategy-when-blending-stacking/107073

Models:
1. NN:
  use binary file to load mmap
  use time delta if not rnn friendly (talkingdata-adtracking-fraud-detection)
  use rnn with batch of 1
  use rankgaussian for normalization (see porto seguro autoencoder)
  use binary files such as np mmap with an holder in the train generator to prevent memory increase
  37 reasons why your NN is not working: blog.slavv.com
  
2. Trees:
  a. Try Dart (drop out) instead of traditional gbtree
  b. custom xgboost objective:
    https://www.kaggle.com/c/higgs-boson/forums/t/10286/customize-loss-function-in-xgboost?page=2
    https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_objective.py
    https://www.kaggle.com/scirpus/prudential-life-insurance-assessment/xgb-test

3. computer vision:
  a. see W2Unet by Dimitry on DSBL 2018.
