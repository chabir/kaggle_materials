{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. count meter_reading equals 0 per site_id, timestamp and meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_df3= pd.read_csv(\"train.csv\")\n",
    "building_df = pd.read_csv(\"building_metadata.csv\")\n",
    "train_df3 = train_df3.merge(building_df[['site_id', 'building_id']], on='building_id', how='left')\n",
    "toto = train_df3.groupby(['site_id','timestamp', 'meter']).meter_reading.transform(lambda x: x[x.eq(0)].size)\n",
    "train_df3['cnt_tmp_meter_siteid'] = toto\n",
    "train_df3.to_feather('train_with_cnt_per_tmp_meter.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. hot encoding of meter per building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a table listing all the building with 4 columns corresponding to the 4 meters. if the building has a partical meter, the value is 1 else it is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_df= pd.read_csv(\"train.csv\")\n",
    "train_df = train_df.set_index(['building_id', 'meter', 'timestamp'])\n",
    "\n",
    "train_df = train_df.unstack(1)\n",
    "train_df.fillna(-1)\n",
    "train_df2 = train_df\n",
    "train_df2.columns = ['meter_reading_0','meter_reading_1','meter_reading_2','meter_reading_3',]\n",
    "train_df2.reset_index(inplace=True)\n",
    "\n",
    "for col in ['meter_reading_0','meter_reading_1','meter_reading_2','meter_reading_3',]:\n",
    "    train_df2[col].loc[~train_df2[col].isnull()]=1\n",
    "    train_df2[col].loc[train_df2[col].isnull()]=0\n",
    "\n",
    "del train_df2['timestamp']\n",
    "\n",
    "train_df3 = train_df2.drop_duplicates(subset=['building_id', 'meter_reading_0','meter_reading_1','meter_reading_2','meter_reading_3',], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "train_df3['meter_reading_0'] = train_df3.groupby('building_id')['meter_reading_0'].transform(max)\n",
    "train_df3['meter_reading_1'] = train_df3.groupby('building_id')['meter_reading_1'].transform(max)\n",
    "train_df3['meter_reading_2'] = train_df3.groupby('building_id')['meter_reading_2'].transform(max)\n",
    "train_df3['meter_reading_3'] = train_df3.groupby('building_id')['meter_reading_3'].transform(max)\n",
    "\n",
    "train_df3 = train_df3.drop_duplicates(subset=['building_id', 'meter_reading_0','meter_reading_1','meter_reading_2','meter_reading_3',], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "train_df3.to_feather('building_all_meters.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. cleaning 0s that are happened at the same time and on the same site (too coincendal to be true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this kernel, Ganfear shows the raw data https://www.kaggle.com/ganfear/missing-data-and-zeros-visualized and visualize when the target (meter_reading) is missing or 0. we can clearly see that there are some vertical blue lines which most likely mean that the 0s were added and especially when these vertical lines happen on several meters, at the same time in the same building on the same site: the goal of the code below is to remove most of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "building_df = pd.read_csv(\"building_metadata.csv\")\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "building_df['cnt_building_per_site'] = building_df.groupby(['site_id']).building_id.transform(lambda x: x.size)\n",
    "train = train.merge(building_df[['building_id', 'cnt_building_per_site', 'site_id']], on='building_id', how='left')\n",
    "\n",
    "print('starting')\n",
    "train['cnt_mreadEQ0_per_tmp_site'] = train.groupby(['timestamp','site_id']).meter_reading.transform(lambda x: x[x.eq(0)].size)\n",
    "print('done 1')\n",
    "train['cnt_mreadEQ0_per_tmp_site_building'] = train.groupby(['timestamp','site_id','building_id']).meter_reading.transform(lambda x: x[x.eq(0)].size)\n",
    "print('done 2')\n",
    "train['sum_meter_reading'] = train.groupby(['building_id', 'meter']).meter_reading.transform('sum')\n",
    "print('done 3')\n",
    "\n",
    "print(train.shape)\n",
    "df_train2 = pd.read_feather('train_with_cnt_per_tmp_meter.feather')\n",
    "train['cnt_tmp_meter_siteid'] = df_train2.cnt_tmp_meter_siteid    ### groupby('site_id','timestamp', 'meter') x[x.eq(0)].size)\n",
    "del df_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "\n",
    "train = train.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-21\")')\n",
    "print(train.shape)\n",
    "\n",
    "train = train[~(((train.cnt_mreadEQ0_per_tmp_site_building>2) & (train.meter_reading==0)))]\n",
    "print(train.shape)\n",
    "\n",
    "train = train[~(((train.cnt_mreadEQ0_per_tmp_site_building==2) & (train.meter_reading==0) & (train.meter==0)))]\n",
    "print(train.shape)\n",
    "\n",
    "train = train[~(((train.site_id==5) &(train.cnt_tmp_meter_siteid/train.cnt_building_per_site>.35) & (train.meter_reading==0)))]\n",
    "print(train.shape)\n",
    "\n",
    "train = train[~(((train.site_id==6) &(train.cnt_tmp_meter_siteid/train.cnt_building_per_site>.35) & (train.meter_reading==0)))]\n",
    "print(train.shape)\n",
    "\n",
    "train = train[~(((train.site_id==8) &(train.cnt_tmp_meter_siteid/train.cnt_building_per_site>.35) & (train.meter_reading==0)))]\n",
    "print(train.shape)\n",
    "\n",
    "train = train[~(((train.site_id==9) &(train.cnt_tmp_meter_siteid/train.cnt_building_per_site>.35) & (train.meter_reading==0)))]\n",
    "print(train.shape)\n",
    "\n",
    "train = train[~(((train.site_id==14) &(train.cnt_tmp_meter_siteid/train.cnt_building_per_site>.35) & (train.meter_reading==0)))]\n",
    "print(train.shape)\n",
    "\n",
    "train = train[~(((train.site_id==15) &(train.cnt_tmp_meter_siteid/train.cnt_building_per_site>.35) & (train.meter_reading==0)))]\n",
    "print(train.shape)\n",
    "\n",
    "train = train[['building_id', 'meter', 'timestamp', 'meter_reading']].reset_index(drop=True)\n",
    "\n",
    "train.to_feather('train_cleanup_001.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. clean up dataset \n",
    "\n",
    "####### code extracted from https://www.kaggle.com/purist1024/ashrae-simple-data-cleanup-lb-1-08-no-leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_is_bad_zero(Xy_subset, min_interval=48, summer_start=3000, summer_end=7500):\n",
    "    \"\"\"Helper routine for 'find_bad_zeros'.\n",
    "    \n",
    "    This operates upon a single dataframe produced by 'groupby'. We expect an \n",
    "    additional column 'meter_id' which is a duplicate of 'meter' because groupby \n",
    "    eliminates the original one.\"\"\"\n",
    "    meter = Xy_subset.meter_id.iloc[0]\n",
    "    is_zero = Xy_subset.meter_reading == 0\n",
    "    if meter == 0:\n",
    "        # Electrical meters should never be zero. Keep all zero-readings in this table so that\n",
    "        # they will all be dropped in the train set.\n",
    "        return is_zero\n",
    "\n",
    "    transitions = (is_zero != is_zero.shift(1))\n",
    "    all_sequence_ids = transitions.cumsum()\n",
    "    ids = all_sequence_ids[is_zero].rename(\"ids\")\n",
    "    if meter in [2, 3]:\n",
    "        # It's normal for steam and hotwater to be turned off during the summer\n",
    "        keep = set(ids[(Xy_subset.timestamp < summer_start) |\n",
    "                       (Xy_subset.timestamp > summer_end)].unique())\n",
    "        is_bad = ids.isin(keep) & (ids.map(ids.value_counts()) >= min_interval)\n",
    "    elif meter == 1:\n",
    "        time_ids = ids.to_frame().join(Xy_subset.timestamp).set_index(\"timestamp\").ids\n",
    "        is_bad = ids.map(ids.value_counts()) >= min_interval\n",
    "\n",
    "        # Cold water may be turned off during the winter\n",
    "        jan_id = time_ids.get(0, False)\n",
    "        dec_id = time_ids.get(8283, False)\n",
    "        if (jan_id and dec_id and jan_id == time_ids.get(500, False) and\n",
    "                dec_id == time_ids.get(8783, False)):\n",
    "            is_bad = is_bad & (~(ids.isin(set([jan_id, dec_id]))))\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected meter type: {meter}\")\n",
    "\n",
    "    result = is_zero.copy()\n",
    "    result.update(is_bad)\n",
    "    return result\n",
    "\n",
    "def find_bad_zeros(X, y):\n",
    "    \"\"\"Returns an Index object containing only the rows which should be deleted.\"\"\"\n",
    "    Xy = X.assign(meter_reading=y, meter_id=X.meter)\n",
    "    is_bad_zero = Xy.groupby([\"building_id\", \"meter\"]).apply(make_is_bad_zero)\n",
    "    return is_bad_zero[is_bad_zero].index.droplevel([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_bad_sitezero` identifies the \"known-bad\" electrical readings from the first 141 days of the data for site 0 (i.e. UCF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_sitezero(X):\n",
    "    \"\"\"Returns indices of bad rows from the early days of Site 0 (UCF).\"\"\"\n",
    "    return X[(X.timestamp < 3378) & (X.site_id == 0) & (X.meter == 0)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_bad_building1099` identifies the most absurdly high readings from building 1099. These are orders of magnitude higher than all data, and have been emperically seen in LB probes to be harmful outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_building1099(X, y):\n",
    "    \"\"\"Returns indices of bad rows (with absurdly high readings) from building 1099.\"\"\"\n",
    "    return X[(X.building_id == 1099) & (X.meter == 2) & (y > 3e4)].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `find_bad_rows` combines all of the above together to allow you to do a one-line cleanup of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_rows(X, y):\n",
    "    return find_bad_zeros(X, y).union(find_bad_sitezero(X)).union(find_bad_building1099(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meteocalc import Temp, dew_point, heat_index, wind_chill, feels_like\n",
    "\n",
    "def c2f(T):\n",
    "    return T * 9 / 5. + 32\n",
    "\n",
    "def windchill(T, v):\n",
    "    return (10*v**.5 - v +10.5) * (33 - T)\n",
    "\n",
    "def prepareweather(df):\n",
    "    df['RH'] = 100 - 5 * (df['air_temperature']-df['dew_temperature'])\n",
    "#     df['RH_above50'] = (df['RH'] > 50).astype(int) \n",
    "    df['heat'] = df.apply(lambda x: heat_index(c2f(x.air_temperature), x.RH).c, axis=1)\n",
    "    df['windchill'] = df.apply(lambda x: windchill(x.air_temperature, x.wind_speed), axis=1)\n",
    "    df['feellike'] = df.apply(lambda x: feels_like(c2f(x.air_temperature), x.RH, x.wind_speed*2.237).c, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def add_lag_feature(weather_df, window=3):  \n",
    "    group_df = weather_df.groupby('site_id')\n",
    "    cols = ['air_temperature', 'dew_temperature', 'heat', 'windchill', 'feellike']\n",
    "    rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "    lag_mean = rolled.mean().reset_index().astype(np.float32)\n",
    "    lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "    lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "    lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "    for col in cols:\n",
    "        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "#         weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "#         weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "#         weather_df[f'{col}_std_lag{window}'] = lag_std[col]\n",
    "\n",
    "\n",
    "def fill_weather_dataset(weather_df):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n",
    "    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_df = pd.concat([weather_df,new_rows])\n",
    "\n",
    "        weather_df = weather_df.reset_index(drop=True) \n",
    "        \n",
    "#     for col in weather_df.columns:\n",
    "#         if col != 'timestamp':\n",
    "#             if weather_df[col].isna().sum():\n",
    "#                 weather_df['na_'+col] = weather_df[col].isna().astype(int)\n",
    "\n",
    "#     weather_df['weath_na_total'] = weather_df.isna().sum(axis=1)\n",
    "    \n",
    "    \n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    # Step 2\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "    weather_df.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather_df.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather_df.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    weather_df = timestamp_align(weather_df)\n",
    "    \n",
    "    print('add heat, RH...')\n",
    "    weather_df = prepareweather(weather_df)\n",
    "    \n",
    "    print('add lag features')\n",
    "    add_lag_feature(weather_df, window=3)\n",
    "    \n",
    "    return weather_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def features_engineering(df):\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df.sort_values(\"timestamp\")\n",
    "    df.reset_index(drop=True)\n",
    "    \n",
    "    # Add more features\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    df[\"month\"] = df[\"timestamp\"].dt.month\n",
    "    df[\"weekday\"] = df[\"timestamp\"].dt.weekday\n",
    "\n",
    "    \n",
    "#     # Remove Unused Columns\n",
    "#     drop = ['sea_level_pressure\", \"wind_direction\", \"wind_speed\", \"precip_depth_1_hr\"]\n",
    "#     df = df.drop(drop, axis=1)\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def building_features(building_meta_df):\n",
    "    building_addfeatures = pd.read_feather('building_all_meters.feather')\n",
    "    \n",
    "    for col in building_meta_df.columns:\n",
    "        if col != 'timestamp':\n",
    "            if building_meta_df[col].isna().sum():\n",
    "                building_meta_df['na_'+col] = building_meta_df[col].isna().astype(int)\n",
    "\n",
    "    building_meta_df['build_na_total'] = building_meta_df.isna().sum(axis=1)\n",
    "\n",
    "    building_meta_df = pd.concat([building_meta_df, \n",
    "                                  building_addfeatures[['meter_reading_0', 'meter_reading_1', \n",
    "                                                        'meter_reading_2', 'meter_reading_3']]], axis=1)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    building_meta_df.primary_use = le.fit_transform(building_meta_df.primary_use)\n",
    "\n",
    "    building_meta_df['cnt_building_per_site'] = building_meta_df.groupby(['site_id']).building_id.transform(lambda x: x.size)\n",
    "    building_meta_df['cnt_building_per_site_prim'] = building_meta_df.groupby(['site_id', 'primary_use']).building_id.transform(lambda x: x.size)\n",
    "    building_meta_df['sqr_mean_per_site'] = building_meta_df.groupby(['site_id', ]).square_feet.transform('median')\n",
    "    building_meta_df['sqr_mean_per_prim_site'] = building_meta_df.groupby(['site_id', 'primary_use']).square_feet.transform('median')\n",
    "\n",
    "    return building_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "import warnings\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "pd.set_option(\"max_columns\", 500)\n",
    "\n",
    "def input_file(file):\n",
    "    path = f\"./{file}\"\n",
    "    if not os.path.exists(path): return path + \".gz\"\n",
    "    return path\n",
    "\n",
    "def compress_dataframe(df):\n",
    "    result = df.copy()\n",
    "    for col in result.columns:\n",
    "        col_data = result[col]\n",
    "        dn = col_data.dtype.name\n",
    "        if dn == \"object\":\n",
    "            result[col] = pd.to_numeric(col_data.astype(\"category\").cat.codes, downcast=\"integer\")\n",
    "        elif dn == \"bool\":\n",
    "            result[col] = col_data.astype(\"int8\")\n",
    "        elif dn.startswith(\"int\") or (col_data.round() == col_data).all():\n",
    "            result[col] = pd.to_numeric(col_data, downcast=\"integer\")\n",
    "        else:\n",
    "            result[col] = pd.to_numeric(col_data, downcast='float')\n",
    "    return result\n",
    "\n",
    "def read_train():\n",
    "    df = pd.read_feather(\"./train_cleanup_001.feather\")\n",
    "    dft = pd.read_csv('test.csv')\n",
    "    \n",
    "#     df = features_engineering(df) ###############################\n",
    "    df.timestamp = (pd.to_datetime(df[\"timestamp\"]) - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    return compress_dataframe(df)\n",
    "\n",
    "def read_building_metadata():\n",
    "     \n",
    "    df = pd.read_csv(input_file(\"building_metadata.csv\"))\n",
    "    df = building_features(df)                 \n",
    "    df = compress_dataframe(df).fillna(-1).set_index(\"building_id\")\n",
    "    return df\n",
    "\n",
    "site_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]\n",
    "\n",
    "def read_weather_train(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    df = pd.read_csv(input_file(\"weather_train.csv\"), parse_dates=[\"timestamp\"])\n",
    "            ################\n",
    "    \n",
    "    print('add heat, RH...')\n",
    "    df = prepareweather(df)\n",
    "    \n",
    "    print('add lag features')\n",
    "    add_lag_feature(df, window=3)\n",
    "            #################\n",
    "    \n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    if fix_timestamps:\n",
    "        GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n",
    "        df.timestamp = df.timestamp + df.site_id.map(GMT_offset_map)\n",
    "    if interpolate_na:\n",
    "        site_dfs = []\n",
    "        for site_id in df.site_id.unique():\n",
    "            # Make sure that we include all possible hours so that we can interpolate evenly\n",
    "            site_df = df[df.site_id == site_id].set_index(\"timestamp\").reindex(range(8784))\n",
    "            site_df.site_id = site_id\n",
    "            for col in [c for c in site_df.columns if c != \"site_id\"]:\n",
    "                if add_na_indicators: site_df[f\"had_{col}\"] = ~site_df[col].isna()\n",
    "                site_df[col] = site_df[col].interpolate(limit_direction='both', method='linear')\n",
    "                # Some sites are completely missing some columns, so use this fallback\n",
    "                site_df[col] = site_df[col].fillna(df[col].median())\n",
    "            site_dfs.append(site_df)\n",
    "        df = pd.concat(site_dfs).reset_index()  # make timestamp back into a regular column\n",
    "    elif add_na_indicators:\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().any(): df[f\"had_{col}\"] = ~df[col].isna()\n",
    "    return compress_dataframe(df).set_index([\"site_id\", \"timestamp\"])\n",
    "\n",
    "def combined_train_data(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    Xy = compress_dataframe(read_train().join(read_building_metadata(), on=\"building_id\").join(\n",
    "        read_weather_train(fix_timestamps, interpolate_na, add_na_indicators),\n",
    "        on=[\"site_id\", \"timestamp\"]).fillna(-1))\n",
    "    return Xy.drop(columns=[\"meter_reading\"]), Xy.meter_reading\n",
    "\n",
    "def _add_time_features(X):\n",
    "    return X.assign(tm_day_of_week=((X.timestamp // 24) % 7), tm_hour_of_day=(X.timestamp % 24))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are simple variants of the ones above, but deal with loading in the test set. They could easily be refactored to share code with those functions, but we keep them separate for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    df = pd.read_csv(input_file(\"test.csv\"), parse_dates=[\"timestamp\"])\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    return compress_dataframe(df).set_index(\"row_id\")\n",
    "\n",
    "def read_weather_test(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    df = pd.read_csv(input_file(\"weather_test.csv\"), parse_dates=[\"timestamp\"])\n",
    "    \n",
    "    ###############\n",
    "    print('add heat, RH...')\n",
    "    df = prepareweather(df)\n",
    "    \n",
    "    print('add lag features')\n",
    "    add_lag_feature(df, window=3)\n",
    "    ##############\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    if fix_timestamps:\n",
    "        GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n",
    "        df.timestamp = df.timestamp + df.site_id.map(GMT_offset_map)\n",
    "    if interpolate_na:\n",
    "        site_dfs = []\n",
    "        for site_id in df.site_id.unique():\n",
    "            # Make sure that we include all possible hours so that we can interpolate evenly\n",
    "            site_df = df[df.site_id == site_id].set_index(\"timestamp\").reindex(range(8784, 26304))\n",
    "            site_df.site_id = site_id\n",
    "            for col in [c for c in site_df.columns if c != \"site_id\"]:\n",
    "                if add_na_indicators: site_df[f\"had_{col}\"] = ~site_df[col].isna()\n",
    "                site_df[col] = site_df[col].interpolate(limit_direction='both', method='linear')\n",
    "                # Some sites are completely missing some columns, so use this fallback\n",
    "                site_df[col] = site_df[col].fillna(df[col].median())\n",
    "            site_dfs.append(site_df)\n",
    "        df = pd.concat(site_dfs).reset_index()  # make timestamp back into a regular column\n",
    "    elif add_na_indicators:\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().any(): df[f\"had_{col}\"] = ~df[col].isna()\n",
    "    return compress_dataframe(df).set_index([\"site_id\", \"timestamp\"])\n",
    "\n",
    "def combined_test_data(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    X = compress_dataframe(read_test().join(read_building_metadata(), on=\"building_id\").join(\n",
    "        read_weather_test(fix_timestamps, interpolate_na, add_na_indicators),\n",
    "        on=[\"site_id\", \"timestamp\"]).fillna(-1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = combined_train_data()\n",
    "\n",
    "bad_rows = find_bad_rows(X, y)\n",
    "pd.Series(bad_rows.sort_values()).to_csv(\"rows_to_drop.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"building_id\", \"meter\", \"site_id\", \"primary_use\", \"had_air_temperature\", \"had_cloud_coverage\",\"had_dew_temperature\", \"had_precip_depth_1_hr\", \"had_sea_level_pressure\", \"had_wind_direction\",\"had_wind_speed\", \"tm_day_of_week\", \"tm_hour_of_day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcol = []\n",
    "\n",
    "#['had_feellike_mean_lag3', 'had_wind_direction', 'had_wind_speed','had_RH', 'had_air_temperature', 'na_floor_count', 'na_year_built',\n",
    "#        'had_heat', 'had_feellike', 'had_air_temperature_mean_lag3',\n",
    "#        'had_dew_temperature_mean_lag3', 'had_heat_mean_lag3', \n",
    "#        'had_windchill', 'had_sea_level_pressure', 'had_dew_temperature',\n",
    "#        'had_windchill_mean_lag3',\"timestamp\"]\n",
    "\n",
    "categorical_columns = [f for f in categorical_columns if f not in dropcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(index=bad_rows)\n",
    "y = y.reindex_like(X)\n",
    "\n",
    "# Additional preprocessing\n",
    "X = compress_dataframe(_add_time_features(X))\n",
    "\n",
    "X = X.drop(dropcol, axis=1)  # Raw timestamp doesn't help when prediction\n",
    "y = np.log1p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = X.copy()\n",
    "XX['meter_reading'] = y.values\n",
    "XX.reset_index(drop=True, inplace=True)\n",
    "XX.to_feather('train_simple_cleanup.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = combined_test_data()\n",
    "Xt = compress_dataframe(_add_time_features(Xt))\n",
    "Xt = Xt.drop(dropcol, axis=1)  # Raw timestamp doesn't help when prediction\n",
    "Xt = Xt.reset_index()\n",
    "Xt.to_feather('test_simple_cleanup.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
