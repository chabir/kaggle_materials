{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main idea for Carvana competition: \n",
    "## A. the pictures are grouped by car to avoid leakage in the validation set\n",
    "## B. background is separated using USV and reused as 4th layer in  \n",
    "## C. the RESIZE of pictures at input or output induces some loss \n",
    "## so the pictures are cut on top and bottom and extended with a black screen on left and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xavierc/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "% matplotlib inline\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.u_net import get_unet_128, get_unet_256, get_unet_512, get_unet_1024, get_unet_tf_1536x1024_crop \\\n",
    "                        ,get_tiram, get_unet_1024_usv, get_unet_tf_1152x1152_usv_crop, get_unet_tf_1280x1280_usv_crop \\\n",
    "                         , get_unet_tf4_1536x1280_usv_bl_crop\n",
    "\n",
    "#input_size = (1536,1024)\n",
    "input_size = (1408,1280)\n",
    "\n",
    "max_epochs = 100\n",
    "batch_size = 3\n",
    "\n",
    "orig_width = 1918\n",
    "orig_height = 1280\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "#model = get_unet_tf_1536x1024_crop()\n",
    "\n",
    "\n",
    "model = get_unet_tf4_1536x1280_usv_bl_crop((1280, 1536, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1408, 1280)\n"
     ]
    }
   ],
   "source": [
    "input_size_w = int(input_size[0])\n",
    "input_size_h = int(input_size[1])\n",
    "epochs = max_epochs\n",
    "\n",
    "print(input_size_w,input_size_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read csv with pictures\n",
    "\n",
    "## the \n",
    "\n",
    "df_train = pd.read_csv('/opt/datasets/kaggle/Carvana/train_masks.csv')\n",
    "ids_train = df_train['img'].map(lambda s: s.split('.')[0])\n",
    "df_train = df_train.sort_values(by='img').reset_index(drop=True)\n",
    "# df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_train['img_path'] = df_train['img'].apply(lambda x: '/opt/datasets/kaggle/Carvana/train/'+x)\n",
    "df_train['mask_path'] = df_train['img'].apply(lambda x: '/opt/datasets/kaggle/Carvana/train_masks_png/'+x.split('.')[0]+'_mask.png')\n",
    "df_train['usv_path'] = df_train['img'].apply(lambda x: '/opt/datasets/kaggle/Carvana/backgdusv2/'+x.split('.')[0]+'.png')\n",
    "df_train= df_train.drop('rle_mask', 1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 4370 samples\n",
      "Validating on 718 samples\n"
     ]
    }
   ],
   "source": [
    "# simple cross validation to start.\n",
    "\n",
    "trn_data = df_train[:4370]\n",
    "val_data = df_train[4370:]\n",
    "\n",
    "print('Training on {} samples'.format(len(trn_data)))\n",
    "print('Validating on {} samples'.format(len(val_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various function to augment the pictures and the mask\n",
    "\n",
    "def randomHueSaturationValue(image, hue_shift_limit=(-180, 180),\n",
    "                             sat_shift_limit=(-255, 255),\n",
    "                             val_shift_limit=(-255, 255), u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(image)\n",
    "        hue_shift = np.random.uniform(hue_shift_limit[0], hue_shift_limit[1])\n",
    "        h = cv2.add(h, hue_shift)\n",
    "        sat_shift = np.random.uniform(sat_shift_limit[0], sat_shift_limit[1])\n",
    "        s = cv2.add(s, sat_shift)\n",
    "        val_shift = np.random.uniform(val_shift_limit[0], val_shift_limit[1])\n",
    "        v = cv2.add(v, val_shift)\n",
    "        image = cv2.merge((h, s, v))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def randomShiftScaleRotate(image, mask,\n",
    "                           shift_limit=(-0.0625, 0.0625),\n",
    "                           scale_limit=(-0.1, 0.1),\n",
    "                           rotate_limit=(-45, 45), aspect_limit=(0, 0),\n",
    "                           borderMode=cv2.BORDER_CONSTANT, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        height, width, channel = image.shape\n",
    "        \n",
    "        add_height = int(height / 3)\n",
    "        add_width = int(width / 3)\n",
    "        image = cv2.copyMakeBorder(image,add_height,add_height,add_width,add_width,cv2.BORDER_REFLECT_101) #top, bottom, left, right \n",
    "        mask = cv2.copyMakeBorder(mask,add_height,add_height,add_width,add_width,cv2.BORDER_REFLECT_101) #top, bottom, left, right \n",
    "        height, width, channel = image.shape\n",
    "        \n",
    "        angle = np.random.uniform(rotate_limit[0], rotate_limit[1])*1.0  # degree\n",
    "        scale = np.random.uniform(1 + scale_limit[0], 1 + scale_limit[1])\n",
    "        aspect = np.random.uniform(1 + aspect_limit[0], 1 + aspect_limit[1])\n",
    "        sx = scale * aspect / (aspect ** 0.5)\n",
    "        sy = scale / (aspect ** 0.5)\n",
    "        dx = round(np.random.uniform(shift_limit[0], shift_limit[1]) * width)\n",
    "        dy = round(np.random.uniform(shift_limit[0], shift_limit[1]) * height)\n",
    "\n",
    "        cc = np.math.cos(angle / 180 * np.math.pi) * sx\n",
    "        ss = np.math.sin(angle / 180 * np.math.pi) * sy\n",
    "        rotate_matrix = np.array([[cc, -ss], [ss, cc]])\n",
    "\n",
    "        box0 = np.array([[0, 0], [width, 0], [width, height], [0, height], ])\n",
    "        box1 = box0 - np.array([width / 2, height / 2])\n",
    "        box1 = np.dot(box1, rotate_matrix.T) + np.array([width / 2 + dx, height / 2 + dy])\n",
    "\n",
    "        box0 = box0.astype(np.float32)\n",
    "        box1 = box1.astype(np.float32)\n",
    "        mat = cv2.getPerspectiveTransform(box0, box1)\n",
    "        image = cv2.warpPerspective(image, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
    "                                    borderValue=(\n",
    "                                        0, 0,\n",
    "                                        0,))\n",
    "        image = image[add_height:-add_height,add_width:-add_width,:]\n",
    "        mask = cv2.warpPerspective(mask, mat, (width, height), flags=cv2.INTER_LINEAR, borderMode=borderMode,\n",
    "                                   borderValue=(\n",
    "                                       0, 0,\n",
    "                                       0,))\n",
    "        mask = mask[add_height:-add_height,add_width:-add_width]\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def randomHorizontalFlip(image, mask, u=0.5):\n",
    "    if np.random.random() < u:\n",
    "        image = cv2.flip(image, 1)\n",
    "        mask = cv2.flip(mask, 1)\n",
    "#     if np.random.random() < u:\n",
    "#         image = cv2.flip(image, 2)\n",
    "#         mask = cv2.flip(mask, 2)\n",
    "#     if np.random.random() < u:\n",
    "#         image = np.rot90(image,1)\n",
    "#         mask = np.rot90(mask,1)\n",
    "#     if np.random.random() < u:\n",
    "#         image = np.rot90(image,2)\n",
    "#         mask = np.rot90(mask,2)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "\n",
    "def mask_jitter(mask):\n",
    "    w,h = mask.shape\n",
    "    v1 = np.random.randint(-1,2)\n",
    "\n",
    "    if v1 ==-1:\n",
    "        mask = cv2.copyMakeBorder(mask,1,0,0,0,cv2.BORDER_REFLECT_101)\n",
    "        mask = mask[:w,:]\n",
    "\n",
    "    if v1 ==1:\n",
    "        mask = cv2.copyMakeBorder(mask,0,1,0,0,cv2.BORDER_REFLECT_101)\n",
    "        mask = mask[1:,:]\n",
    "\n",
    "        \n",
    "    v2 = np.random.randint(-1,2)\n",
    "    if v2 ==-1:\n",
    "        mask = cv2.copyMakeBorder(mask,0,0,1,0,cv2.BORDER_REFLECT_101)\n",
    "        mask = mask[:,:h]\n",
    "\n",
    "    if v2==1:\n",
    "        mask = cv2.copyMakeBorder(mask,0,0,0,1,cv2.BORDER_REFLECT_101)\n",
    "        mask = mask[:,1:]\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "from dehaze import DarkChannel, AtmLight, TransmissionEstimate, TransmissionRefine, Recover\n",
    "\n",
    "def dehaze_img(img):\n",
    "    I = img.astype('float64')/255.;\n",
    "\n",
    "    dark = DarkChannel(I,15);\n",
    "    A = AtmLight(I,dark);\n",
    "    te = TransmissionEstimate(I,A,15);\n",
    "    t = TransmissionRefine(img,te);\n",
    "    J = Recover(I,t,A,0.1);\n",
    "    return np.array(J*255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## customized generator for both train and test\n",
    "\n",
    "\n",
    "def train_generator(data, batch_size):\n",
    "\n",
    "    while 1:\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for i_batch in range(batch_size):\n",
    "            i_line = np.random.randint(len(data))\n",
    "            \n",
    "            img = cv2.imread(data.img_path.iloc[i_line])\n",
    "#             img = cv2.fastNlMeansDenoisingColored(img,None,5,5,5,5)\n",
    "            img = cv2.resize(img, (input_size_w, input_size_h),cv2.INTER_AREA)\n",
    "            img = cv2.copyMakeBorder(img,0,0,64,64,cv2.BORDER_REFLECT_101) #top, bottom, left, right\n",
    "            usv = cv2.imread(data.usv_path.iloc[i_line], cv2.IMREAD_GRAYSCALE)\n",
    "            usv = cv2.resize(usv, (input_size_w, input_size_h),cv2.INTER_AREA)\n",
    "            usv = cv2.copyMakeBorder(usv,0,0,64,64,cv2.BORDER_REFLECT_101)\n",
    "            \n",
    "            mask = cv2.imread(data.mask_path.iloc[i_line], cv2.IMREAD_GRAYSCALE)\n",
    "#             if mask.max() <=1:\n",
    "#                 mask[mask<.5]=0\n",
    "#                 mask[mask>.5]=255\n",
    "# #                 mask = mask*255\n",
    "            mask = cv2.resize(mask, (input_size_w, input_size_h) ,cv2.INTER_AREA)\n",
    "            mask = cv2.copyMakeBorder(mask,0,0,64,64,cv2.BORDER_REFLECT_101)\n",
    "            \n",
    "            img = randomHueSaturationValue(img,\n",
    "                                           hue_shift_limit=(-1, 1),\n",
    "                                           sat_shift_limit=(-1, 1),\n",
    "                                           val_shift_limit=(-1, 1))\n",
    "            \n",
    "            img, mask = randomShiftScaleRotate(img, mask,\n",
    "                                               shift_limit=(-0.03, 0.03),\n",
    "                                               scale_limit=(-0.00, 0.00),\n",
    "                                               rotate_limit=(-0,0))\n",
    "            \n",
    "#             imgdeh = DarkChannel(img,9)\n",
    "    \n",
    "            #### concatenate img and usv in a 4D tensor\n",
    "            img = np.dstack((img, usv)) \n",
    "            \n",
    "            \n",
    "            img, mask = randomHorizontalFlip(img, mask)\n",
    "            \n",
    "            mask = mask[:,64:-64]\n",
    "#             mask = mask_jitter(mask)\n",
    "            mask = np.expand_dims(mask, axis=2)\n",
    "\n",
    "            \n",
    "            x_batch.append(img)\n",
    "            y_batch.append(mask)\n",
    "            #print(mask.shape, mask) \n",
    "        x_batch = np.array(x_batch, np.float32) /255. - .5\n",
    "        y_batch = np.array(y_batch, np.float32) / 255\n",
    "#         x_batch = x_batch.transpose((0,3,1,2))\n",
    "#         y_batch = y_batch.transpose((0,3,1,2))\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def valid_generator(data):\n",
    "    \n",
    "    while 1:\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for i_batch in range(batch_size):\n",
    "            i_line = np.random.randint(len(data))\n",
    "            \n",
    "            img = cv2.imread(data.img_path.iloc[i_line])\n",
    "#             img = cv2.fastNlMeansDenoisingColored(img,None,5,5,5,5)\n",
    "            img = cv2.resize(img, (input_size_w, input_size_h),cv2.INTER_AREA)\n",
    "            img = cv2.copyMakeBorder(img,0,0,64,64,cv2.BORDER_REFLECT_101)\n",
    "        \n",
    "            usv = cv2.imread(data.usv_path.iloc[i_line], cv2.IMREAD_GRAYSCALE)\n",
    "            usv = cv2.resize(usv, (input_size_w, input_size_h))\n",
    "            usv = cv2.copyMakeBorder(usv,0,0,64,64,cv2.BORDER_REFLECT_101)\n",
    "            \n",
    "#             imgdeh = DarkChannel(img,9)\n",
    "            \n",
    "            img = np.dstack((img, usv))\n",
    "            \n",
    "            mask = cv2.imread(data.mask_path.iloc[i_line], cv2.IMREAD_GRAYSCALE)\n",
    "#             if mask.max() <=1:\n",
    "#                 mask[mask<.5]=0\n",
    "#                 mask[mask>.5]=255\n",
    "            mask = cv2.resize(mask, (input_size_w, input_size_h),cv2.INTER_AREA)\n",
    "            mask = cv2.copyMakeBorder(mask,0,0,64,64,cv2.BORDER_REFLECT_101)\n",
    "            \n",
    "            mask = mask[:,64:-64]\n",
    "            \n",
    "            mask = np.expand_dims(mask, axis=2)\n",
    "#             img = dehaze_img(img)\n",
    "\n",
    "            x_batch.append(img)\n",
    "            y_batch.append(mask)\n",
    "        \n",
    "        x_batch = np.array(x_batch, np.float32) /255. - .5\n",
    "        y_batch = np.array(y_batch, np.float32) / 255\n",
    "#         x_batch = x_batch.transpose((0,3,1,2))\n",
    "#         y_batch = y_batch.transpose((0,3,1,2))\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## customized generator to potentially use test results for pseudo labeling method.\n",
    "## can also be used to rebalance the train set\n",
    "\n",
    "def trn_gen_mix(data, prop=0.67):\n",
    "\n",
    "    n1 = int(batch_size*prop)\n",
    "    n2 = batch_size - n1\n",
    "    sg1 = train_generator(trn_data, n1)\n",
    "    sg2 = train_generator(df_test, n2)\n",
    "    while True:\n",
    "        out1 = sg1.next()\n",
    "        out2 = sg2.next()\n",
    "        if out2 is None:\n",
    "            yield out1\n",
    "        else:\n",
    "            yield np.concatenate((out1[0], out2[0])), np.concatenate((out1[1], out2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_dice_metric',\n",
    "                           patience=6,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='max'),\n",
    "             ReduceLROnPlateau(monitor='val_dice_metric',\n",
    "                               factor=0.1,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               epsilon=5e-2,\n",
    "                               mode='max'),\n",
    "             ModelCheckpoint(monitor='val_dice_metric',\n",
    "                             filepath='/home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/best_weights_1280x1280_unet_th_usv_crop_t0053b.hdf5',\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='max'),\n",
    "            TensorBoard(log_dir='logs')]\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "model.load_weights('/home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/best_weights_1280x1280_unet_th_usv_crop_t0053b.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1457/1457 [==============================] - 1766s - loss: -0.9895 - dice_metric: 0.9947 - val_loss: -0.9900 - val_dice_metric: 0.9950\n",
      "Epoch 2/100\n",
      "1457/1457 [==============================] - 1753s - loss: -0.9896 - dice_metric: 0.9948 - val_loss: -0.9900 - val_dice_metric: 0.9950\n",
      "Epoch 3/100\n",
      "1457/1457 [==============================] - 1702s - loss: -0.9899 - dice_metric: 0.9949 - val_loss: -0.9902 - val_dice_metric: 0.9951\n",
      "Epoch 4/100\n",
      "1457/1457 [==============================] - 1700s - loss: -0.9900 - dice_metric: 0.9950 - val_loss: -0.9901 - val_dice_metric: 0.9950\n",
      "Epoch 5/100\n",
      "1456/1457 [============================>.] - ETA: 1s - loss: -0.9899 - dice_metric: 0.9949\n",
      "Epoch 00004: reducing learning rate to 9.99999974738e-07.\n",
      "1457/1457 [==============================] - 1698s - loss: -0.9899 - dice_metric: 0.9949 - val_loss: -0.9900 - val_dice_metric: 0.9950\n",
      "Epoch 6/100\n",
      "1457/1457 [==============================] - 1697s - loss: -0.9899 - dice_metric: 0.9949 - val_loss: -0.9903 - val_dice_metric: 0.9951\n",
      "Epoch 7/100\n",
      "1457/1457 [==============================] - 1696s - loss: -0.9901 - dice_metric: 0.9950 - val_loss: -0.9901 - val_dice_metric: 0.9950\n",
      "Epoch 8/100\n",
      "1456/1457 [============================>.] - ETA: 1s - loss: -0.9901 - dice_metric: 0.9950\n",
      "Epoch 00007: reducing learning rate to 9.99999997475e-08.\n",
      "1457/1457 [==============================] - 1595s - loss: -0.9901 - dice_metric: 0.9950 - val_loss: -0.9904 - val_dice_metric: 0.9952\n",
      "Epoch 9/100\n",
      "1457/1457 [==============================] - 1558s - loss: -0.9900 - dice_metric: 0.9950 - val_loss: -0.9904 - val_dice_metric: 0.9952\n",
      "Epoch 10/100\n",
      "1457/1457 [==============================] - 1558s - loss: -0.9900 - dice_metric: 0.9950 - val_loss: -0.9902 - val_dice_metric: 0.9950\n",
      "Epoch 11/100\n",
      "1456/1457 [============================>.] - ETA: 1s - loss: -0.9901 - dice_metric: 0.9950\n",
      "Epoch 00010: reducing learning rate to 1.00000001169e-08.\n",
      "1457/1457 [==============================] - 1713s - loss: -0.9901 - dice_metric: 0.9950 - val_loss: -0.9906 - val_dice_metric: 0.9953\n",
      "Epoch 12/100\n",
      "1457/1457 [==============================] - 1699s - loss: -0.9901 - dice_metric: 0.9950 - val_loss: -0.9903 - val_dice_metric: 0.9951\n",
      "Epoch 13/100\n",
      "1457/1457 [==============================] - 1700s - loss: -0.9901 - dice_metric: 0.9950 - val_loss: -0.9905 - val_dice_metric: 0.9952\n",
      "Epoch 14/100\n",
      "1456/1457 [============================>.] - ETA: 1s - loss: -0.9902 - dice_metric: 0.9951\n",
      "Epoch 00013: reducing learning rate to 9.99999993923e-10.\n",
      "1457/1457 [==============================] - 1703s - loss: -0.9902 - dice_metric: 0.9951 - val_loss: -0.9903 - val_dice_metric: 0.9951\n",
      "Epoch 15/100\n",
      "1457/1457 [==============================] - 1702s - loss: -0.9903 - dice_metric: 0.9951 - val_loss: -0.9905 - val_dice_metric: 0.9952\n",
      "Epoch 16/100\n",
      "1457/1457 [==============================] - 1701s - loss: -0.9903 - dice_metric: 0.9951 - val_loss: -0.9902 - val_dice_metric: 0.9951\n",
      "Epoch 17/100\n",
      "1456/1457 [============================>.] - ETA: 1s - loss: -0.9902 - dice_metric: 0.9951\n",
      "Epoch 00016: reducing learning rate to 9.99999971718e-11.\n",
      "1457/1457 [==============================] - 1701s - loss: -0.9902 - dice_metric: 0.9951 - val_loss: -0.9907 - val_dice_metric: 0.9953\n",
      "Epoch 18/100\n",
      "1457/1457 [==============================] - 1704s - loss: -0.9903 - dice_metric: 0.9951 - val_loss: -0.9907 - val_dice_metric: 0.9953\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5211a05810>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, Adam, Adamaccum, SGDAccum\n",
    "model.optimizer=Adamaccum(lr=1e-5, decay=1e-6)\n",
    "\n",
    "model.fit_generator(generator=train_generator(trn_data,batch_size),\n",
    "                    steps_per_epoch=np.ceil(float(len(trn_data)) / float(batch_size)),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(val_data),\n",
    "                    validation_steps=np.ceil(float(len(val_data)) / float(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Epoch 00012: reducing learning rate to 9.99999974738e-07.\n",
    "# 1018/1018 [==============================] - 1292s - loss: -0.9937 - dice_metric: 0.9968 - val_loss: -0.9936 - val_dice_metric: 0.9968\n",
    "# Epoch 00012: early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple fold -- test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_path = '/home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/'\n",
    "\n",
    "model1 = get_unet_tf_1280x1280_usv_crop()\n",
    "model1.load_weights(weights_path+'unettfcv1280x1289_crop_fold_1.hdf5')\n",
    "model2 = get_unet_tf_1280x1280_usv_crop()\n",
    "model2.load_weights(weights_path+'unettfcv1280x1289_crop_fold_2.hdf5')\n",
    "model3 = get_unet_tf_1280x1280_usv_crop()\n",
    "model3.load_weights(weights_path+'unettfcv1280x1289_crop_fold_3.hdf5')\n",
    "model4 = get_unet_tf_1280x1280_usv_crop()\n",
    "model4.load_weights(weights_path+'unettfcv1280x1289_crop_fold_4.hdf5')\n",
    "model5 = get_unet_tf_1280x1280_usv_crop()\n",
    "model5.load_weights(weights_path+'unettfcv1280x1289_crop_fold_5.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carvana_path = '/opt/datasets/kaggle/Carvana/'\n",
    "# testmasks_fld =  sorted(glob.glob(carvana_path + 'test/*'))\n",
    "# len(testmasks_fld), testmasks_fld[:2]\n",
    "\n",
    "# df_test = pd.DataFrame()\n",
    "# df_test['img_path'] = testmasks_fld\n",
    "# df_test['img'] = df_test['img_path'].apply(lambda x: x.split('/')[-1])\n",
    "# df_test['mask_path'] = df_test['img'].apply(lambda x: '/opt/datasets/kaggle/Carvana/test_masks/'+x+'.png')\n",
    "# df_test = df_test.sort_values(by='img').reset_index(drop=True)\n",
    "# # df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# df_test['usv_path'] = df_test['img'].apply(lambda x: '/opt/datasets/kaggle/Carvana/backgdusv2/'+x.split('.')[0]+'.png')\n",
    "\n",
    "# df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/33355 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load test sub file...\n",
      "load model weights...\n",
      "model is ready...\n",
      "Predicting on 100064 samples with batch_size = 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 1370/33355 [45:54<17:52:08,  2.01s/it]"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import multiprocessing as queue\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "weight_path = '/home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/'\n",
    "\n",
    "print('load test sub file...')\n",
    "df_test = pd.read_csv('/opt/datasets/kaggle/Carvana/sample_submission.csv')\n",
    "ids_test = df_test['img'].map(lambda s: s.split('.')[0])\n",
    "\n",
    "names = []\n",
    "for id in ids_test:\n",
    "    names.append('{}.jpg'.format(id))\n",
    "\n",
    "\n",
    "# https://www.kaggle.com/stainsby/fast-tested-rle\n",
    "def run_length_encode(mask):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    inds = mask.flatten()\n",
    "    runs = np.where(inds[1:] != inds[:-1])[0] + 2\n",
    "    runs[1::2] = runs[1::2] - runs[:-1:2]\n",
    "    rle = ' '.join([str(r) for r in runs])\n",
    "    return rle\n",
    "\n",
    "# model.load_weights('/home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/best_weights_1280x1280_unet_tf_usv_crop_t0053b.hdf5')\n",
    "\n",
    "rles = []\n",
    "\n",
    "print('load model weights...')\n",
    "\n",
    "# model.load_weights(filepath=weight_path + 'best_weights_1280x1280_unet_tf_usv_crop_t0053b.hdf5')\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "print('model is ready...')\n",
    "\n",
    "q_size = 10\n",
    "\n",
    "\n",
    "def data_loader(q, ):\n",
    "    for start in range(0, len(ids_test), batch_size):\n",
    "        x_batch = []\n",
    "        end = min(start + batch_size, len(ids_test))\n",
    "        ids_test_batch = ids_test[start:end]\n",
    "        for id in ids_test_batch.values:\n",
    "            \n",
    "            img_path = '/opt/datasets/kaggle/Carvana/test/{}.jpg'.format(id)\n",
    "            usv_path = '/opt/datasets/kaggle/Carvana/backgdusv2/{}.png'.format(id)\n",
    "            \n",
    "            img = cv2.imread(img_path)\n",
    "            \n",
    "#             img = cv2.fastNlMeansDenoisingColored(img,None,5,5,5,5)\n",
    "            img = cv2.resize(img, (input_size_w, input_size_h),cv2.INTER_AREA)\n",
    "            img = cv2.copyMakeBorder(img,64,64,64,64,cv2.BORDER_REFLECT_101)\n",
    "        \n",
    "            usv = cv2.imread(usv_path, cv2.IMREAD_GRAYSCALE)\n",
    "            usv = cv2.resize(usv, (input_size_w, input_size_h),cv2.INTER_AREA)\n",
    "            usv = cv2.copyMakeBorder(usv,64,64,64,64,cv2.BORDER_REFLECT_101)\n",
    "            \n",
    "            img = np.dstack((img, usv))\n",
    "            \n",
    "            x_batch.append(img)\n",
    "        x_batch = np.array(x_batch, np.float32) /255. - .5\n",
    "        q.put(x_batch)\n",
    "\n",
    "\n",
    "def predictor(q, ):\n",
    "    for i in tqdm(range(0, len(ids_test), batch_size)):\n",
    "        x_batch = q.get()\n",
    "        with graph.as_default():\n",
    "            preds1 = model1.predict(x_batch, batch_size=batch_size)\n",
    "            preds2 = model2.predict(x_batch, batch_size=batch_size)\n",
    "            preds3 = model3.predict(x_batch, batch_size=batch_size)\n",
    "            preds4 = model4.predict(x_batch, batch_size=batch_size)\n",
    "            preds5 = model5.predict(x_batch, batch_size=batch_size)\n",
    "            preds = (preds1 + preds2 + preds3 + preds4 + preds5)/5.\n",
    "        preds = np.squeeze(preds, axis=3)\n",
    "        for pred in preds:\n",
    "            prob = cv2.resize(pred, (orig_width, orig_height), cv2.INTER_LINEAR)\n",
    "            mask = prob > .5 ##################\n",
    "            rle = run_length_encode(mask)\n",
    "            rles.append(rle)\n",
    "\n",
    "\n",
    "q = queue.Queue(maxsize=q_size)\n",
    "t1 = threading.Thread(target=data_loader, name='DataLoader', args=(q,))\n",
    "t2 = threading.Thread(target=predictor, name='Predictor', args=(q,))\n",
    "print('Predicting on {} samples with batch_size = {} '.format(len(ids_test), batch_size))\n",
    "t1.start()\n",
    "t2.start()\n",
    "# Wait for both threads to finish\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "print(\"Generating submission file...\")\n",
    "df = pd.DataFrame({'img': names, 'rle_mask': rles})\n",
    "df.to_csv('./submit/submission0053b_1024tfusv_9970_5fold.csv.gz', index=False, compression='gzip')\n",
    "print(\"C'est FINI !!!...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### cross validation ### unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = '/home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/'\n",
    "HISTORY_FOLDER_PATH = '/home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/historycv/'\n",
    "nb_folds = 5\n",
    "batch_size = 3\n",
    "\n",
    "def train_single_model(num_fold, trn_index, val_index, files):\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "    from keras.optimizers import Adam, SGD\n",
    "\n",
    "    print('Creating and compiling UNET...')\n",
    "    \n",
    "    model.load_weights('/home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/best_weights_1280x1280_unet_tf_usv_crop_t0053b.hdf5')\n",
    "\n",
    "\n",
    "    final_model_path = MODELS_PATH + 'unettfcv1280x1289_crop_fold_{}.hdf5'.format(num_fold)\n",
    "    if os.path.isfile(final_model_path):\n",
    "        print('Model already exists for fold {}. Skipping !!!!!!!!!!'.format(final_model_path))\n",
    "        return 0.0\n",
    "\n",
    "    cache_model_path = MODELS_PATH + 'unettfcv1280x1289_crop_fold_{}.hdf5'.format(num_fold)\n",
    "    if os.path.isfile(cache_model_path) and restore:\n",
    "        print('Load model from last point: ', cache_model_path)\n",
    "        model.load_weights(cache_model_path)\n",
    "    else:\n",
    "        print('Start training from begining')\n",
    "\n",
    "#     if optim_type == 'SGD':\n",
    "#         optim = SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#     else:\n",
    "#         optim = Adam(lr=learning_rate, decay=1e-6)\n",
    "#     model.compile(optimizer=optim, loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    print('Fitting model...')\n",
    "    df_trn = df_train.ix[trn_index, :].reset_index(drop=True)\n",
    "    df_val = df_train.ix[val_index, :].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    print('Batch size: {}'.format(batch_size))\n",
    "    samples_train_per_epoch = np.ceil(float(len(df_trn)) / float(batch_size))\n",
    "    samples_valid_per_epoch = np.ceil(float(len(df_val)) / float(batch_size))\n",
    "    \n",
    "    print('Samples train: {}, Samples valid: {}'.format(samples_train_per_epoch, samples_valid_per_epoch))\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_dice_metric',\n",
    "                           patience=3,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='max'),\n",
    "             ReduceLROnPlateau(monitor='val_dice_metric',\n",
    "                               factor=0.1,\n",
    "                               patience=2,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='max'),\n",
    "        ModelCheckpoint(cache_model_path, monitor='val_dice_metric', save_best_only=True, verbose=0)]\n",
    "\n",
    "    \n",
    "    training_gen = train_generator(df_trn)\n",
    "    validation_gen = valid_generator(df_val)\n",
    "\n",
    "    from keras.optimizers import RMSprop, Adam\n",
    "    model.optimizer=Adamaccum(lr=1e-6, decay=1e-7, accumulator=10)\n",
    "\n",
    "    history = model.fit_generator(generator=train_generator(df_trn),\n",
    "                    steps_per_epoch=np.ceil(float(len(df_trn)) / float(batch_size)),\n",
    "                    epochs=6,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_generator(df_val),\n",
    "                    validation_steps=np.ceil(float(len(df_val)) / float(batch_size)))\n",
    "\n",
    "    min_loss = min(history.history['val_loss'])\n",
    "    print('Minimum loss for given fold: ', min_loss)\n",
    "    model.load_weights(cache_model_path)\n",
    "    model.save(final_model_path)\n",
    "    now = datetime.datetime.now()\n",
    "#     filename = HISTORY_FOLDER_PATH + 'history_{}_{:.4f}_lr_{}_{}.csv'.format(num_fold, min_loss, learning_rate, now.strftime(\"%Y-%m-%d-%H-%M\"))\n",
    "#     pd.DataFrame(history.history).to_csv(filename, index=False)\n",
    "    return min_loss\n",
    "\n",
    "\n",
    "def run_cross_validation_create_models_unet(nfolds=5):\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    files = df_train\n",
    "\n",
    "    kf = KFold(n_splits=nfolds, shuffle=False, random_state=66)\n",
    "    num_fold = 0\n",
    "    sum_score = 0\n",
    "    for trn_index, val_index in kf.split(range(len(files))):\n",
    "        num_fold += 1\n",
    "        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "        print('Split train: ', len(trn_index))\n",
    "        print('Split valid: ', len(val_index))\n",
    "        score = train_single_model(num_fold, trn_index, val_index, files)\n",
    "        print score\n",
    "        sum_score += score\n",
    "\n",
    "    print('Avg loss: {}'.format(sum_score/nfolds))\n",
    "    \n",
    "    return sum_score/nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start KFold number 1 from 5\n",
      "('Split train: ', 4070)\n",
      "('Split valid: ', 1018)\n",
      "Creating and compiling UNET...\n",
      "Model already exists for fold /home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/unettfcv1280x1289_crop_fold_1.hdf5. Skipping !!!!!!!!!!\n",
      "0.0\n",
      "Start KFold number 2 from 5\n",
      "('Split train: ', 4070)\n",
      "('Split valid: ', 1018)\n",
      "Creating and compiling UNET...\n",
      "Model already exists for fold /home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/unettfcv1280x1289_crop_fold_2.hdf5. Skipping !!!!!!!!!!\n",
      "0.0\n",
      "Start KFold number 3 from 5\n",
      "('Split train: ', 4070)\n",
      "('Split valid: ', 1018)\n",
      "Creating and compiling UNET...\n",
      "Model already exists for fold /home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/unettfcv1280x1289_crop_fold_3.hdf5. Skipping !!!!!!!!!!\n",
      "0.0\n",
      "Start KFold number 4 from 5\n",
      "('Split train: ', 4071)\n",
      "('Split valid: ', 1017)\n",
      "Creating and compiling UNET...\n",
      "Model already exists for fold /home/xavierc/Documents/Carvana/Kaggle-Carvana-Image-Masking-Challenge-master/weights/unettfcv1280x1289_crop_fold_4.hdf5. Skipping !!!!!!!!!!\n",
      "0.0\n",
      "Start KFold number 5 from 5\n",
      "('Split train: ', 4071)\n",
      "('Split valid: ', 1017)\n",
      "Creating and compiling UNET...\n",
      "Start training from begining\n",
      "Fitting model...\n",
      "Batch size: 3\n",
      "Samples train: 1357.0, Samples valid: 339.0\n",
      "Epoch 1/6\n",
      "1357/1357 [==============================] - 1901s - loss: -0.9933 - dice_metric: 0.9967 - val_loss: -0.9931 - val_dice_metric: 0.9965\n",
      "Epoch 2/6\n",
      "1357/1357 [==============================] - 1866s - loss: -0.9935 - dice_metric: 0.9967 - val_loss: -0.9937 - val_dice_metric: 0.9968\n",
      "Epoch 3/6\n",
      " 443/1357 [========>.....................] - ETA: 1130s - loss: -0.9937 - dice_metric: 0.9968"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-59fdbb9ab2c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_cross_validation_create_models_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-d086cb78f1f7>\u001b[0m in \u001b[0;36mrun_cross_validation_create_models_unet\u001b[0;34m(nfolds)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Split train: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Split valid: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_single_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0msum_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-d086cb78f1f7>\u001b[0m in \u001b[0;36mtrain_single_model\u001b[0;34m(num_fold, trn_index, val_index, files)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                     validation_steps=np.ceil(float(len(df_val)) / float(batch_size)))\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mmin_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1838\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1839\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_cross_validation_create_models_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
